create virtual environment: python -m venv .venv
Activate virtual environment: .venv\Scripts\activate
How to activate .env file: pip install python-dotenv then from dotenv import load_dotenv then add this function load_dotenv()  # Load variables from .env into the environment
Run the requirement.txt file: pip install -r requirements.txt
create requirement.txt file: pip freeze > requirements.txt
to run the program : streamlit run main.py

GIT PUSH Steps: 
In the terminal, initialize a new Git repository: git init
Add all files in your folder to the staging area: git add .
Commit the files with a message: git commit -m "Initial commit"
Link your local repository to the GitHub repository you mentioned (first time required): git remote add origin https://github.com/tosurajitc/Agentic_Ticket_Anaolysis.git
Push the changes to the remote repository: git push -u origin master (or master)

If git hub fails then:
git remote rename origin old-origin
git remote add origin https://github.com/tosurajitc/Agentic_Ticket_Anaolysis.git

Streamlit: https://github.com/tosurajitc/Agentic_Ticket_Anaolysis/blob/master/streamlit_app.py

ticket-analysis-project/
│
├── agents/
│   ├── __init__.py
│   ├── analysis_agent.py         # Core analysis logic
│   ├── automation_agent.py       # Generates automation recommendations
│   ├── chat_agent.py             # Handles conversational queries
│   ├── data_agent.py             # Handles data loading and preprocessing
│   ├── qa_agent.py               # Generates qualitative answers
│   ├── supervisor_agent.py       # Coordinates between different agents
│   └── visualization_agent.py    # Creates data visualizations
│
├── utils/
│   ├── __init__.py
│   ├── json_utils.py             # Custom JSON serialization
│   ├── rate_limiter.py           # API rate limiting utility
│   └── batch_processor.py        # Batch processing utility
│
├── app.py                        # Streamlit application entry point
└── .env                          # Environment configuration

# Ticket Data Analysis and Automation Insight Generation

The solution processes large ticket datasets (100,000+ rows) through a coordinated system of specialized agents 
that work together to extract insights and identify automation opportunities.

When processing begins, the **DataProcessingAgent** loads, cleans, and chunks the data into manageable segments. 
It handles missing values and standardizes column names, making the data ready for analysis.

Next, the **AnalysisAgent** performs a comprehensive examination of the data, analyzing basic statistics, 
time dimensions, categories, priorities, and status distributions. It also examines text fields using LLM-based 
analysis to extract common themes and patterns from ticket descriptions.

The **VisualizationAgent** creates visual representations of the data patterns, identifying meaningful columns 
and generating charts that highlight key distributions and relationships in the data.

For automation insights, the **AutomationRecommendationAgent** examines patterns in categories, text fields, 
time data, and workflows. It identifies repetitive patterns that suggest automation opportunities, 
then uses the LLM to generate specific, actionable automation recommendations with implementation plans.

The **QualitativeAnswerAgent** generates answers to strategic questions about the data, providing deeper 
insights into automation potential by analyzing patterns that may not be immediately obvious from statistical analysis alone.

All these agents are orchestrated by the **SupervisorAgent**, which manages the workflow, handles errors, 
and ensures each agent has the necessary data. To handle large datasets efficiently, the system employs:

1. Data chunking to process data in manageable batches
2. Rate limiting to manage API calls to the LLM
3. Parallel processing where appropriate
4. Robust error handling with fallback mechanisms
5. JSON serialization utilities to handle complex data types

The result is a comprehensive analysis that identifies automation opportunities by detecting patterns in 
large volumes of ticket data, producing practical, data-driven recommendations for improving operational efficiency.
